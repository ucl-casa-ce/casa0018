{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPhbft0WIKysS8tzVUfa61A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ucl-casa-ce/casa0018/blob/main/Week7/anomaly_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Up\n",
        "\n",
        "First set up the necessary Python imports and set up Tensor Board which provides a visual output of the training process."
      ],
      "metadata": {
        "id": "krUuPwlrNRAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, RepeatVector, TimeDistributed\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.utils import timeseries_dataset_from_array\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "7it5S6lPNk4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Data\n",
        "\n",
        "Create sine function data. We’ll use the NumPy linspace to generate x values ranging between 0 and 200*2*Pi (200 cycles) and NumPy sine function to generate sine values to the corresponding x. We subdivide the range into 10000 data points.\n",
        "\n",
        "We then add some noise to the data. Finally, we visualize the data."
      ],
      "metadata": {
        "id": "TO4OU_ouNtYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.linspace(0, 200*2*np.pi, 10001) # in radians, 200 cycles, 50 data points per cycle\n",
        "y = np.sin(x)\n",
        "y += 0.05*np.random.randn(len(y)) #Add some noise to the data to make it more realistic\n",
        "\n",
        "# Inject anomalies (random spikes)\n",
        "num_anomalies = 10\n",
        "anomaly_indices = np.random.choice(2000, num_anomalies, replace=False)\n",
        "y[anomaly_indices+8000] += np.random.uniform(-5, 5, num_anomalies)  # Add large spikes\n",
        "\n",
        "# Plot our data\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.plot(range(len(y)), y)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wUhsAUMvOYME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Divide the data set into windows of data"
      ],
      "metadata": {
        "id": "p5SWwe4Uwll1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length = 50 # sequence length - the length of the training window\n",
        "\n",
        "data = []\n",
        "for i in range(len(y) - sequence_length):\n",
        "    data.append(y[i : i + sequence_length])\n",
        "\n",
        "Y = np.array(data).reshape(-1, sequence_length, 1)  # Reshape for LSTM\n",
        "print(len(Y))"
      ],
      "metadata": {
        "id": "VbIH-0O_PDry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a model\n",
        "\n",
        "The code to create a LSTM is similar to that for earlier NNs you have already seen.\n",
        "\n",
        "The variable (n_features) defined stands for the number of features in the training data i.e., as we are dealing with univariate data we’ll only have one feature whereas if we are using multivariate data containing multiple features then we must specify the count of features in our data."
      ],
      "metadata": {
        "id": "Flu-yhlEQbMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_features = 1\n",
        "latent_dim = 50\n",
        "batch_size = 16\n",
        "\n",
        "# Encoder\n",
        "encoder = Sequential()\n",
        "encoder.add(LSTM(latent_dim, activation=\"relu\", return_sequences=False, input_shape=(sequence_length, n_features)))\n",
        "encoder.add(RepeatVector(sequence_length)) # Repeat for decoder input\n",
        "\n",
        "# Decoder\n",
        "decoder = Sequential()\n",
        "decoder.add(LSTM(latent_dim, activation=\"relu\", return_sequences=True)) # Pass the output shape of the encoder\n",
        "decoder.add(TimeDistributed(Dense(1)))\n",
        "\n",
        "# Autoencoder\n",
        "autoencoder = Sequential([encoder, decoder])\n",
        "autoencoder.compile(loss='mse', optimizer='adam')\n",
        "autoencoder.summary()"
      ],
      "metadata": {
        "id": "1Dd7a2btQbws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train\n",
        "Train the autoencoder on 80% 0f the data. Set aside 20% for testing"
      ],
      "metadata": {
        "id": "mtPtDaQKTcjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Train Autoencoder on Normal Data ====\n",
        "Y_train = Y[:8000]  # Train on 80% of data\n",
        "\n",
        "Y_test = Y[8000:]\n",
        "\n",
        "autoencoder.fit(Y_train, Y_train, epochs=20, batch_size=batch_size, validation_split=0.1)\n"
      ],
      "metadata": {
        "id": "iiIpmZJ-TcDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing Predictions Using The Test Data\n",
        "\n",
        "Now let's compare the predictions of the LSTM model using our test data set.\n"
      ],
      "metadata": {
        "id": "HMfQnKgjWQIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==== Anomaly Detection ====\n",
        "Y_pred = autoencoder.predict(Y_test)  # Reconstruct sequences\n",
        "reconstruction_errors = np.mean(np.abs(Y_test - Y_pred), axis=(1, 2))  # Compute reconstruction error per sequence\n",
        "\n",
        "# Set Anomaly Threshold (e.g., 95th percentile)\n",
        "threshold = np.percentile(reconstruction_errors, 95)\n",
        "anomalies = reconstruction_errors > threshold  # Flag anomalies\n",
        "\n",
        "\n",
        "# ==== Plot Results ====\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(range(len(y[8000:])), y[8000:], label=\"Original Data\")\n",
        "\n",
        "# Extract the first element of each sequence in Y_pred\n",
        "Y_pred_first_element = Y_pred[:, 0, 0]  # Shape will be (1951,)\n",
        "\n",
        "# Plot the first element of each predicted sequence against the original data\n",
        "plt.plot(range(len(Y_pred_first_element)), Y_pred_first_element, label=\"Reconstructed Data\")\n",
        "#last_sequence_pred = Y_pred[-1, :, 0]  # Shape will be (50,)\n",
        "#plt.plot(range(1951,2001), last_sequence_pred, color='orange')\n",
        "\n",
        "plt.scatter(np.where(anomalies)[0] + sequence_length, y[8000 + sequence_length:][anomalies], color=\"red\", label=\"Anomalies\")\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel(\"Time Step\")\n",
        "plt.ylabel(\"Value\")\n",
        "plt.title(\"Anomaly Detection in Sinusoidal Time Series\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8HKU6GjAWS9z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
